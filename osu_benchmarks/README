OMB (OSU Micro-Benchmarks)
--------------------------
MPI-1
-----
osu_bibw            - Bidirectional Bandwidth Test
osu_bw              - Bandwidth Test
osu_latency         - Latency Test
osu_mbw_mr          - Multiple Bandwidth / Message Rate Test
osu_multi_lat       - Multi-pair Latency Test
osu_allgather       - MPI_Allgather Latency Test
osu_allgatherv      - MPI_Allgatherv Latency Test
osu_allreduce       - MPI_Allreduce Latency Test
osu_alltoall        - MPI_Alltoall Latency Test
osu_alltoallv       - MPI_Alltoallv Latency Test
osu_barrier         - MPI_Barrier Latency Test
osu_bcast           - MPI_Bcast Latency Test
osu_gather          - MPI_Gather Latency Test
osu_gatherv         - MPI_Gatherv Latency Test
osu_reduce          - MPI_Reduce Latency Test
osu_reduce_scater   - MPI_Reduce_scatter Latency Test
osu_scatter         - MPI_Scatter Latency Test
osu_scatterv        - MPI_Scatterv Latency Test

MPI-2
-----
osu_acc_latency         - Latency Test for Accumulate with Active 
                          Synchronization
osu_get_bw              - Bandwidth Test for Get with Active Synchronization
osu_get_latency         - Latency Test for Get with Active Synchronization
osu_latency_mt          - Multi-threaded Latency Test
osu_passive_acc_latency - Latency Test for Accumulate with Passive 
                          Synchronization
osu_passive_get_bw      - Bandwidth Test for Get with Passive Synchronization
osu_passive_get_latency - Latency Test for Get with Passive Synchronization
osu_passive_put_bw      - Bandwidth Test for Put with Passive Synchronization
osu_passive_put_latency - Latency Test for Put with Passive Synchronization
osu_put_bibw            - Bi-directional Bandwidth Test for Put with Active 
                          Synchronization
osu_put_bw              - Bandwidth Test for Put with Active Synchronization
osu_put_latency         - Latency Test for Put with Active Synchronization

Latency Test
    * The latency tests are carried out in a ping-pong fashion. The sender
    * sends a message with a certain data size to the receiver and waits for a
    * reply from the receiver. The receiver receives the message from the sender
    * and sends back a reply with the same data size. Many iterations of this
    * ping-pong test are carried out and average one-way latency numbers are
    * obtained. Blocking version of MPI functions (MPI_Send and MPI_Recv) are
    * used in the tests. This test is available here.

Multi-threaded Latency Test (MPI-2 Benchmark [requires threading support])
    * The multi-threaded latency test performs a ping-pong test with a single
    * sender process and multiple threads on the receiving process. In this test
    * the sending process sends a message of a given data size to the receiver
    * and waits for a reply from the receiver process. The receiving process has
    * a variable number of receiving threads (set by default to 2), where each
    * thread calls MPI_Recv and upon receiving a message sends back a response
    * of equal size. Many iterations are performed and the average one-way
    * latency numbers are reported. This test is available here.

Bandwidth Test
    * The bandwidth tests were carried out by having the sender sending out a
    * fixed number (equal to the window size) of back-to-back messages to the
    * receiver and then waiting for a reply from the receiver. The receiver
    * sends the reply only after receiving all these messages. This process is
    * repeated for several iterations and the bandwidth is calculated based on
    * the elapsed time (from the time sender sends the first message until the
    * time it receives the reply back from the receiver) and the number of bytes
    * sent by the sender. The objective of this bandwidth test is to determine
    * the maximum sustained date rate that can be achieved at the network level.
    * Thus, non-blocking version of MPI functions (MPI_Isend and MPI_Irecv) were
    * used in the test. This test is available here.

Bidirectional Bandwidth Test
    * The bidirectional bandwidth test is similar to the bandwidth test, except
    * that both the nodes involved send out a fixed number of back-to-back
    * messages and wait for the reply. This test measures the maximum
    * sustainable aggregate bandwidth by two nodes. This test is available here.

Multiple Bandwidth / Message Rate test
    * The multi-pair bandwidth and message rate test evaluates the aggregate
    * uni-directional bandwidth and message rate between multiple pairs of
    * processes. Each of the sending processes sends a fixed number of messages
    * (the window size) back-to-back to the paired receiving process before
    * waiting for a reply from the receiver. This process is repeated for
    * several iterations. The objective of this benchmark is to determine the
    * achieved bandwidth and message rate from one node to another node with a
    * configurable number of processes running on each node. The test is
    * available here.
    *
    * The Multiple Bandwidth / Message Rate Test (osu_mbw_mr) is intended to be
    * used with block assigned ranks.  This means that all processes on the
    * same machine are assigned ranks sequentially.
    * 
    * Rank	Block   Cyclic
    * ----------------------
    * 0	host1	host1
    * 1	host1	host2
    * 2	host1	host1
    * 3	host1	host2
    * 4	host2	host1
    * 5	host2	host2
    * 6	host2	host1
    * 7	host2	host2
    * 
    * If you're using mpirun_rsh with mvapich the ranks are assigned in the
    * order they are seen in the hostfile or on the command line.  Please see
    * your process managers' documentation for information on how to control
    * the distribution of the rank to host mapping.

Multi-pair Latency Test
    * This test is very similar to the latency test. However, at the same 
    * instant multiple pairs are performing the same test simultaneously.
    * In order to perform the test across just two nodes the hostnames must
    * be specified in block fashion.

Latency Test for Put with Active Synchronization (MPI-2 Benchmark)
    * Post-Wait/Start-Complete synchronization is used in this test. The origin
    * process calls MPI_Put to directly place data of a certain size in the
    * remote process's window. It waits on the MPI_Win_complete call to ensure
    * the local completion of the message. The target process calls
    * MPI_Win_wait to make sure the message has been received in its window.
    * Then the origin and target are interchanged and the communication happens
    * in the opposite direction.  Several iterations of this test is carried
    * out and the average put latency numbers is obtained. The latency includes
    * the synchronization time. By default, the window memory is allocated in
    * shared memory by providing 'alloc_shm' hint to MPI_Alloc_mem call. This
    * hint is specific to MVAPICH2 and is ignored by other MPI libraries.
    * MVAPICH2 takes advantage of this to optimize intra-node one-sided
    * communication. It does not have an impact on internode performance. This
    * optimization can be disabled by providing '-n' or '-no-hints' option to
    * the benchmark.

Latency Test for Get with Active Synchronization (MPI-2 Benchmark)
    * Post-Wait/Start-Complete synchronization is used in this test. The origin
    * process calls MPI_Get to directly fetch data of a certain size from the
    * target process's window into a local buffer. It then waits on a
    * synchronization call (MPI_Win_complete) for local completion of the Gets.
    * The remote process waits on a MPI_Win_wait call. After the
    * synchronization calls, the target and origin process are switched for a
    * message in the opposite direction.  Several iterations of this test are
    * carried out and the average get latency numbers is obtained. The latency
    * includes the synchronization time. By default, the window memory is
    * allocated in shared memory by providing 'alloc_shm' hint to MPI_Alloc_mem
    * call. This hint is specific to MVAPICH2 and is ignored by other MPI
    * libraries. MVAPICH2 takes advantage of this to optimize intra-node
    * one-sided communication. It does not have an impact on internode
    * performance. This optimization can be disabled by providing '-n' or
    * '-no-hints' option to the benchmark. 

Bandwidth Test for Put with Active Synchronization (MPI-2 Benchmark)
    * Post-Wait/Start-Complete synchronization is used in this test. The test
    * is carried out by the origin process calling a fixed number of
    * back-to-back MPI_Puts on remote window and then waiting on a
    * synchronization call (MPI_Win_complete) for their completion. The remote
    * process participates in synchronization with MPI_Win_post and
    * MPI_Win_wait calls. This process is repeated for several iterations and
    * the bandwidth is calculated based on the elapsed time and the number of
    * bytes put by the origin process.  By default, the window memory is
    * allocated in shared memory by providing 'alloc_shm' hint to MPI_Alloc_mem
    * call. This hint is specific to MVAPICH2 and is ignored by other MPI
    * libraries. MVAPICH2 takes advantage of this to optimize intra-node
    * one-sided communication. It does not have an impact on internode
    * performance. This optimization can be disabled by providing '-n' or
    * '-no-hints' option to the benchmark. 

Bandwidth Test for Get with Active Synchronization (MPI-2 Benchmark)
    * Post-Wait/Start-Complete synchronization is used in this test. The test
    * is carried out by origin process calling a fixed number of back-to-back
    * MPI_Gets and then waiting on a synchronization call (MPI_Win_complete)
    * for their completion. The remote process participates in synchronization
    * with MPI_Win_post and MPI_Win_wait calls. This process is repeated for
    * several iterations and the bandwidth is calculated based on the elapsed
    * time and the number of bytes received by the origin process. By default,
    * the window memory is allocated in shared memory by providing 'alloc_shm'
    * hint to MPI_Alloc_mem call. This hint is specific to MVAPICH2 and is
    * ignored by other MPI libraries.  MVAPICH2 takes advantage of this to
    * optimize intra-node one-sided communication.  It does not have an impact
    * on internode performance. This optimization can be disabled by providing
    * '-n' or '-no-hints' option to the benchmark. 

Bidirectional Bandwidth Test for Put with Active Synchronization (MPI-2
  Benchmark)
    * Post-Wait/Start-Complete synchronization is used in this test. This test
    * is similar to the bandwidth test, except that both the processes involved
    * send out a fixed number of back-to-back MPI_Puts and wait for their
    * completion.  This test measures the maximum sustainable aggregate
    * bandwidth by two processes. By default, the window memory is allocated in
    * shared memory by providing 'alloc_shm' hint to MPI_Alloc_mem call. This
    * hint is specific to MVAPICH2 and is ignored by other MPI libraries.
    * MVAPICH2 takes advantage of this to optimize intra-node one-sided
    * communication. It does not have an impact on internode performance. This
    * optimization can be disabled by providing '-n' or '-no-hints' option to
    * the benchmark. 

Latency Test for Accumulate with Active Synchronization (MPI-2 Benchmark)
    * Post-Wait/Start-Complete synchronization is used in this test. The origin
    * process calls MPI_Accumulate to combine data from the local buffer with
    * the data in the remote window and store it in the remote window. The
    * combining operation used in the test is MPI_SUM. The origin process then
    * waits on a synchronization call (MPI_Win_complete) for local completion
    * of the operations.  The remote process waits on a MPI_Win_wait call.
    * After the synchronization call, the target and origin process are
    * switched for the pong message. Several iterations of this test are
    * carried out and the average accumulate latency number is obtained. The
    * latency includes the synchronization time. By default, the window memory
    * is allocated in shared memory by providing 'alloc_shm' hint to
    * MPI_Alloc_mem call. This hint is specific to MVAPICH2 and is ignored by
    * other MPI libraries. MVAPICH2 takes advantage of this to optimize
    * intra-node one-sided communication. It does not have an impact on
    * internode performance. This optimization can be disabled by providing
    * '-n' or '-no-hints' option to the benchmark. 

Latency Test for Put with Passive Synchronization (MPI-2 Benchmark)
    * The origin process calls MPI_Win_lock to lock the target process's window
    * and calls MPI_Put to directly place data of certain size in the window.
    * Then it calls MPI_Win_unlock to ensure completion of the Put and release
    * lock on the window. This is carried out for several iterations and the
    * average time for MPI_Lock + MPI_Put + MPI_Unlock calls is measured.  By
    * default, the window memory is allocated in shared memory by providing
    * 'alloc_shm' hint to MPI_Alloc_mem call. This optimization is specific to
    * MVAPICH2 and the hint is ignored by other MPI libraries. It does not have
    * an impact on internode performance. This optimization can be disabled by
    * providing '-n' or '-no-hints' option to the benchmark. 

Latency Test for Get with Passive Synchronization (MPI-2 Benchmark)
    * The origin process calls MPI_Win_lock to lock the target process's window
    * and calls MPI_Get to directly read data of certain size from the window.
    * Then it calls MPI_Win_unlock to ensure completion of the Get and releases
    * lock on remote window. This is carried out for several iterations and the
    * average time for MPI_Lock + MPI_Get + MPI_Unlock calls is measured.  By
    * default, the window memory is allocated in shared memory by providing
    * 'alloc_shm' hint to MPI_Alloc_mem call. This optimization is specific to
    * MVAPICH2 and the hint is ignored by other MPI libraries. It does not have
    * an impact on internode performance. This optimization can be disabled by
    * providing '-n' or '-no-hints' option to the benchmark. 

Bandwidth Test for Put with Passive Synchronization (MPI-2 Benchmark)
    * The origin process calls MPI_Win_lock to lock the target process's window
    * and calls a fixed number of back-to-back MPI_Puts to directly place data
    * in the window. Then it calls MPI_Win_unlock to ensure completion of the
    * Puts and release lock on remote window. This process is repeated for
    * several iterations and the bandwidth is calculated based on the elapsed
    * time and the number of bytes put by the origin process. By default, the
    * window memory is allocated in shared memory by providing 'alloc_shm' hint
    * to MPI_Alloc_mem call. This optimization is specific to MVAPICH2 and the
    * hint is ignored by other MPI libraries. It does not have an impact on
    * internode performance. This optimization can be disabled by providing
    * '-n' or '-no-hints' option to the benchmark.

Bandwidth Test for Get with Passive Synchronization (MPI-2 Benchmark)
    * The origin process calls MPI_Win_lock to lock the target process's window
    * and calls a fixed number of back-to-back MPI_Gets to directly get data
    * from the window. Then it calls MPI_Win_unlock to ensure completion of the
    * Gets and release lock on the window. This process is repeated for several
    * iterations and the bandwidth is calculated based on the elapsed time and
    * the number of bytes read by the origin process. By default, the window
    * memory is allocated in shared memory by providing 'alloc_shm' hint to
    * MPI_Alloc_mem call. This optimization is specific to MVAPICH2 and the
    * hint is ignored by other MPI libraries. It does not have an impact on
    * internode performance. This optimization can be disabled by providing
    * '-n' or '-no-hints' option to the benchmark.

Latency Test for Accumulate with Passive Synchronization (MPI-2 Benchmark)
    * The origin process calls MPI_Win_lock to lock the target process's window
    * and calls MPI_Accumulate to combine data from a local buffer with the
    * data in the remote window and store it in the remote window. Then it
    * calls MPI_Win_unlock to ensure completion of the Accumulate and release
    * lock on the window. This is carried out for several iterations and the
    * average time for MPI_Lock + MPI_Accumulate + MPI_Unlock calls is
    * measured.  By default, the window memory is allocated in shared memory by
    * providing 'alloc_shm' hint to MPI_Alloc_mem call. This optimization is
    * specific to MVAPICH2 and the hint is ignored by other MPI libraries. It
    * does not have an impact on internode performance.  This optimization can
    * be disabled by providing '-n' or '-no-hints' option to the benchmark.

Collective Latency Tests
    * The latest OMB Version includes benchmarks for various MPI collective
    * operations (MPI_Allgather, MPI_Alltoall, MPI_Allreduce, MPI_Barrier,
    * MPI_Bcast, MPI_Gather, MPI_Reduce, MPI_Reduce_Scatter, MPI_Scatter and
    * vector collectives). These benchmarks work in the following manner.
    * Suppose users run the osu_bcast benchmark with N processes, the benchmark
    * measures the min, max and the average latency of the MPI_Bcast collective
    * operation across N processes, for various message lengths, over a large
    * number of iterations. In the default version, these benchmarks report the
    * average latency for each message length. Additionally, the benchmarks
    * offer the following options: 
    * "-f" can be used to report additional statistics of the benchmark,
           such as min and max latencies and the number of iterations. 
    * "-m" option can be used to set the maximum message length to be used in a
           benchmark. In the default version, the benchmarks report the
           latencies for up to 1MB message lengths. 
    * "-i" can be used to set the number of iterations to run for each message
           length. 
    * "-M" can be used to set per process maximum memory consumption.  By
           default the benchmarks are limited to 512MB allocations.


CUDA and OpenACC Extensions to OMB
----------------------------------
CUDA Extensions to OMB are enabled when MVAPICH2 is configured with CUDA
support.  OpenACC Extensions are enabled when MVAPICH2 detects OpenACC support
at configure time.  The following benchmarks have been extended to evaluate
performance of MPI communication using buffers on NVIDIA GPU devices. 

    osu_bibw        - Bidirectional Bandwidth Test
    osu_bw          - Bandwidth Test
    osu_latency     - Latency Test
    osu_alltoall    - MPI_Alltoall Latency Test
    osu_gather      - MPI_Gather Latency Test
    osu_scatter     - MPI_Scatter Latency Test

If both CUDA and OpenACC support is enabled you can switch between the modes
using the -d [cuda|openacc] option to the benchmarks.  Whether a process
allocates its communication buffers on the GPU device or on the host can be
controlled at run-time.  Use the -h option for more help.

    ./osu_latency -h
    Usage: osu_latency [options] [RANK0 RANK1]

    RANK0 and RANK1 may be `D' or `H' which specifies whether
    the buffer is allocated on the accelerator device or host
    memory for each mpi rank

    options:
      -d TYPE   accelerator device buffers can be of TYPE `cuda' or `openac'
      -h        print this help message

Each of the pt2pt benchmarks takes two input parameters. The first parameter
indicates the location of the buffers at rank 0 and the second parameter
indicates the location of the buffers at rank 1. The value of each of these
parameters can be either 'H' or 'D' to indicate if the buffers are to be on the
host or on the device respectively. When no parameters are specified, the
buffers are allocated on the host.  The collective benchmarks will use buffers
allocated on the device if the -d option is used otherwise the buffers will be
allocated on the host.

Examples:

    - mpirun_rsh -np 2 -hostfile hostfile MV2_USE_CUDA=1 osu_latency D D

In this run, the latency test allocates buffers at both rank 0 and rank 1 on
the GPU devices.

    - mpirun_rsh -np 2 -hostfile hostfile MV2_USE_CUDA=1 osu_bw D H

In this run, the bandwidth test allocates buffers at rank 0 on the GPU device
and buffers at rank 1 on the host.

Setting GPU affinity
--------------------
GPU affinity for processes should be set before MPI_Init is called.  The process
rank on a node is normally used to do this and different MPI launchers expose
this information through different environment variables. The benchmarks use an
environment variable called LOCAL_RANK to get this information. 

A script like below can be used to export this environment variable when using
mpirun_rsh.  This can be adapted to work with other MPI launchers and libraries.

    #!/bin/bash

    export LOCAL_RANK=$MV2_COMM_WORLD_LOCAL_RANK
    exec $*

A copy of this script is installed as get_local_rank alongside the benchmarks.
It can be used as follows: 

    mpirun_rsh -np 2 -hostfile hostfile MV2_USE_CUDA=1 get_local_rank \
        ./osu_latency D D

